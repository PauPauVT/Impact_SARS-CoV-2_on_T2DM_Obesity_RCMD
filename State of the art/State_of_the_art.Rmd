---
title: 'State of the Art'
author: "Pau Vendrell"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document: default
  html_document:
    df_print: paged
  word_document: default
---

Necessary packages

```{r}
library(readODS)
library(rcrossref)
library(dplyr)
library("tm")
library("SnowballC")
library("wordcloud")
library("RColorBrewer")
library(kableExtra)
library(compareGroups)
library(FactoMineR)
library(Matrix)
library(factoextra)
library("corrplot")
library(igraph)
library(cluster)
library(FNN)
library(NMF)
```

Charge the data

```{r}
df <- read_ods(here::here("dois.ods"))
```

Clean the database

```{r}
for (i in 1:nrow(df)) df[i, "dois"] <- trimws(df[i, "dois"])

df <- na.omit(df)

df["title"] <- NA
df["abstracts"] <- NA
df["referenced_by"] <- NA
```

Import the titles and abstracts of the papers

```{r, warning=FALSE, eval=FALSE}
for (i in 1:nrow(df)) {
  article_info <- cr_works(df[i,"dois"])
  df[i,"title"] <- ifelse(is.null(article_info$data$title), NA, article_info$data$title)
  df[i,"abstracts"] <- ifelse(is.null(article_info$data$abstract), NA, article_info$data$abstract)
  df[i,"referenced_by"] <- ifelse(is.null(article_info$data$is.referenced.by.count), NA, article_info$data$is.referenced.by.count)
}
```

```{r}
# saveRDS(df, file = "dois_title_abstract.rds")
```

```{r}
df <- readRDS(here::here("dois_title_abstract.rds"))
```

```{r}
df$abstracts <- as.character(df$abstracts)
df$referenced_by <- as.numeric(df$referenced_by)
```

# Analysis of the Titles

Wordcloud of the Titles:

```{r, warning = FALSE}
# dataset dels títols
df_titols <- df[!is.na(df$title),]

my_text_variable <- iconv(df_titols$title, from = "UTF-8", to = "ASCII//TRANSLIT")

my_corpus <- Corpus(VectorSource(my_text_variable), readerControl = list(language = "english", encoding = "UTF-8"))

# Preprocessing steps
my_corpus <- tm_map(my_corpus, content_transformer(function(x) gsub("<jats:title>|Abstract|</jats:title>|<jats:sec>|OBJECTIVE", " ", x)))
my_corpus <- tm_map(my_corpus, content_transformer(tolower)) # Convert to lowercase

# agrupació de sinònims de covid, diabetis tipus 2 i diabetis tipus 1

my_corpus <- tm_map(my_corpus, content_transformer(function(x) gsub("coronavirus|covid-19|covid 19|sars-cov-2|sars cov 2|severe acute respiratory syndrome coronavirus 2|2019-ncov|sars coronavirus 2|coronavirus disease-19|coronavirus disease 19|coronavirus, disease 19|2019 novel coronavirus infection", "covid", x)))

my_corpus <- tm_map(my_corpus, content_transformer(function(x) gsub("2 diabetes|type 2 diabetes|t2d|dm2|diabetes mellitus type 2|diabetes mellitus type ii|t2dm|diabetes type 2|diabetes type ii|type ii diabetes|diabetes, type 2|diabetes, type ii|diabetes mellitus, type 2|diabetes mellitus, type ii|tdm|type 2 diabetes mellitus|type 2 diabetes Mellitus|type two diabetes|diabetes type two", "tiidm", x)))

y_corpus <- tm_map(my_corpus, content_transformer(function(x) gsub("type 1 diabetes|t1d|dm1|diabetes mellitus type 1|diabetes mellitus type i|t1dm|diabetes type 1|diabetes type i|type i diabetes|diabetes, type 1|diabetes, type i|diabetes mellitus, type 1|diabetes mellitus, type i", "tidm", x)))

my_corpus <- tm_map(my_corpus, content_transformer(function(x) gsub("long covid|post covid|post coronavirus", "postcovid", x)))

my_corpus <- tm_map(my_corpus, removePunctuation) # Remove punctuation
my_corpus <- tm_map(my_corpus, removeNumbers) # Remove numbers
my_corpus <- tm_map(my_corpus, removeWords, stopwords("english"))
my_corpus <- tm_map(my_corpus, removeWords, stopwords("spanish"))
my_corpus <- tm_map(my_corpus, content_transformer(function(x) gsub("[/|!]", " ", x))) # Remove special characters
my_corpus <- tm_map(my_corpus, stripWhitespace)
my_corpus <- tm_map(my_corpus, stemDocument)

my_corpus <- tm_map(my_corpus, content_transformer(function(x) gsub("mellitus|patient|studi|sever|increas|use|outcom|pandem|associ|jatssec|diseas|compar|mnmn|momo|xmlnshttpwwwworgmathmathml|among|jatsinlineformula|jatsitalicpjatsital|may|case|peopl|scpcovidscp|type|factor|ace|level|effect|ratio|result|chang|activ|includ|signific|due|follow|without|respect|need|show|find|two|evalu|era|dpp|recommend|take|learn|also|larg|acut|yet|howev|aim|dka|total|high|higher|mechan|thpi|can|adrenerg|areg|bbc|dcr|birc|clublink|ddit|ecm|egr|enclos|erbb|focal|foxo|gadda|glipr|gpba|hippo|icam|jakstat|mthfd|myc|nfkb|nfkbia|parl|pikakt|pluropot|pmaip|relb|sesn|sqk|smarcd|stat|stem|tcr|tgf|tlr|tox|wnt|zoonot|kinas|member|mtor|prkaa|slca|cero|marco|mas|nivel|medento|asahiii|cavit|iklebsiella|cavit|man", "", x)))

title_corpus <- my_corpus

wordcloud(words = title_corpus,
          min.freq = 10, # Minimum word frequency to include
          scale = c(3, 0.5), # Word size scaling
          colors = brewer.pal(8, "Dark2")) # Color palette
title("TITLES WORDCLOUD")
```

```{r}
# transformació del Corpus en matriu de freqüències absolutes

m <- as.data.frame(t(as.matrix(TermDocumentMatrix(my_corpus))))
dim(m)
```

```{r, warning = FALSE}
#### Hopkins test
library(hopkins)
set.seed(123)
hopkins(m)

# a value close to 1 tends to indicate the data is highly clustered
# so now we know that our data is highly clustered
```

```{r}
library(clValid)

summary(clValid(m, 3:8, clMethods=c("hierarchical","kmeans","pam"), validation="internal"))
```

```{r}
# matriu de distàncies amb la distàncai de canberra

dist_matrix <- dist(x=m, method = "canberra")
```

Gradient TESS wit k-menas:

```{r}
TESS <- fviz_nbclust(m, FUNcluster= kmeans, diss=NULL, k.max=15, method = "wss")

# gradient TESS:

gt<-data.frame()

gt[1,1]<-(TESS$data[1,2]-TESS$data[2,2])*100/TESS$data[1,2]
gt[2,1]<-(TESS$data[2,2]-TESS$data[3,2])*100/TESS$data[2,2]
gt[3,1]<-(TESS$data[3,2]-TESS$data[4,2])*100/TESS$data[3,2]
gt[4,1]<-(TESS$data[4,2]-TESS$data[5,2])*100/TESS$data[4,2]
gt[5,1]<-(TESS$data[5,2]-TESS$data[6,2])*100/TESS$data[5,2]
gt[6,1]<-(TESS$data[6,2]-TESS$data[7,2])*100/TESS$data[6,2]
gt[7,1]<-(TESS$data[7,2]-TESS$data[8,2])*100/TESS$data[7,2]
gt[8,1]<-(TESS$data[8,2]-TESS$data[9,2])*100/TESS$data[8,2]
gt[9,1]<-(TESS$data[9,2]-TESS$data[10,2])*100/TESS$data[9,2]

plot(2:10,gt[,1],type="b",xlab="clusters", ylab="GradTess",main="TESS")
```

We observe that the optimal number of clusters is 3.

Gradient TESS with PAM:

```{r}
TESS3 <- fviz_nbclust(m, FUNcluster= pam, diss=NULL, k.max=15, method = "wss")

# gradient TESS

gt<-data.frame()

gt[1,1]<-(TESS3$data[1,2]-TESS3$data[2,2])*100/TESS3$data[1,2]
gt[2,1]<-(TESS3$data[2,2]-TESS3$data[3,2])*100/TESS3$data[2,2]
gt[3,1]<-(TESS3$data[3,2]-TESS3$data[4,2])*100/TESS3$data[3,2]
gt[4,1]<-(TESS3$data[4,2]-TESS3$data[5,2])*100/TESS3$data[4,2]
gt[5,1]<-(TESS3$data[5,2]-TESS3$data[6,2])*100/TESS3$data[5,2]
gt[6,1]<-(TESS3$data[6,2]-TESS3$data[7,2])*100/TESS3$data[6,2]
gt[7,1]<-(TESS3$data[7,2]-TESS3$data[8,2])*100/TESS3$data[7,2]
gt[8,1]<-(TESS3$data[8,2]-TESS3$data[9,2])*100/TESS3$data[8,2]
gt[9,1]<-(TESS3$data[9,2]-TESS3$data[10,2])*100/TESS3$data[9,2]

plot(2:10,gt[,1],type="b",xlab="clusters", ylab="GradTess",main="TESS")
```

We observe that the optimal number of clusters is 3.

PAM method applied with k=3:

```{r}
k <- 3
pam.k <- pam(dist_matrix,k,diss=T)

fviz_cluster(list(data = m, cluster = pam.k$clustering),ellipse.type = "convex", repel = TRUE, show.clust.cent = TRUE, ggtheme = theme_minimal())
```


```{r}
for (i in 1:k) {
 cat("10 mots més repetits en el cluster ",i," : ", names(sort(colSums(m[pam.k$clustering==i,]), decreasing = TRUE)[1:20]),"\n") 
}

for (i in 1:k) {
  word_frequencies <- colSums(m[pam.k$clustering==i,])
  word_freq_df <- data.frame(word = names(word_frequencies), freq = word_frequencies)

  wordcloud(words = word_freq_df$word,
            freq = word_freq_df$freq,
            min.freq = 10, # Minimum word frequency to include
            scale = c(3, 0.5), # Word size scaling
            colors = brewer.pal(8, "Dark2")) # Color palette
}
```

Mostrem la informació dels centroides de l'anàlisis dels títols:

```{r}
df_titols[pam.k$medoids,]
```

# Analysis of the Abstracts

Wordcloud of the Abstracts:

```{r, warning = FALSE}
# dataset dels àbstarcts
df_abstracts <- df[!is.na(df$abstracts),]

my_text_variable <- iconv(df_abstracts$abstracts, from = "UTF-8", to = "ASCII//TRANSLIT")

my_corpus <- Corpus(VectorSource(my_text_variable), readerControl = list(language = "english", encoding = "UTF-8"))

# Preprocessing steps
my_corpus <- tm_map(my_corpus, content_transformer(function(x) gsub("<jats:title>|Abstract|</jats:title>|<jats:sec>|OBJECTIVE|<jats:p>|</jats:p>|<scp>|</scp>", "", x)))
#inspect(my_corpus)

my_corpus <- tm_map(my_corpus, content_transformer(tolower)) # Convert to lowercase

my_corpus <- tm_map(my_corpus, content_transformer(function(x) gsub("covid-19 syndrome|coronavirus|covid-19|covid 19|sars-cov-2|severe acute respiratory syndrome coronavirus 2|2019-ncov|sars coronavirus 2|coronavirus disease-19|coronavirus disease 19|coronavirus, disease 19|2019 novel coronavirus infection|coronavirus-2|covid disease 2019|coronavirus disease 2019", "covid", x)))

my_corpus <- tm_map(my_corpus, content_transformer(function(x) gsub("long covid|post covid|post coronavirus", "postcovid", x)))

my_corpus <- tm_map(my_corpus, content_transformer(function(x) gsub("2 diabetes|type 2 diabetes|t2d|dm2|diabetes mellitus type 2|diabetes mellitus type ii|t2dm|diabetes type 2|diabetes type ii|type ii diabetes|diabetes, type 2|diabetes, type ii|diabetes mellitus, type 2|diabetes mellitus, type ii|tdm|type 2 diabetes mellitus|type 2 diabetes Mellitus|type two diabetes|diabetes type two", "tiidm", x)))

my_corpus <- tm_map(my_corpus, content_transformer(function(x) gsub("types 1|type 1 diabetes|t1d|dm1|diabetes mellitus type 1|diabetes mellitus type i|t2dm|diabetes type 1|diabetes type i|type i diabetes|diabetes, type 1|diabetes, type i|diabetes mellitus, type 1|diabetes mellitus, type i|type 1 diabetes mellitus|type 1", "tidm", x)))

my_corpus <- tm_map(my_corpus, removePunctuation) # Remove punctuation
my_corpus <- tm_map(my_corpus, removeNumbers) # Remove numbers
my_corpus <- tm_map(my_corpus, removeWords, stopwords("english"))
my_corpus <- tm_map(my_corpus, removeWords, stopwords("spanish"))
my_corpus <- tm_map(my_corpus, content_transformer(function(x) gsub("[/|!]", "", x))) # Remove special characters
my_corpus <- tm_map(my_corpus, stripWhitespace)
my_corpus <- tm_map(my_corpus, stemDocument)
my_corpus <- tm_map(my_corpus, content_transformer(function(x) gsub("mellitus|patient|studi|sever|increas|use|outcom|pandem|associ|jatssec|diseas|compar|mnmn|momo|xmlnshttpwwwworgmathmathml|among|jatsinlineformula|jatsitalicpjatsital|may|case|peopl|scpcovidscp|type|factor|ace|level|effect|ratio|result|chang|activ|includ|signific|due|follow|without|respect|need|show|find|two|evalu|era|dpp|recommend|take|learn|also|larg|acut|yet|howev|aim|dka|total|high|higher|mechan|thpi|can|adrenerg|areg|bbc|dcr|birc|clublink|ddit|ecm|egr|enclos|erbb|focal|foxo|gadda|glipr|gpba|hippo|icam|jakstat|mthfd|myc|nfkb|nfkbia|parl|pikakt|pluropot|pmaip|relb|sesn|sqk|smarcd|stat|stem|tcr|tgf|tlr|tox|wnt|zoonot|kinas|member|mtor|prkaa|slca|cero|marco|mas|nivel|medento|asahiii|cavit|iklebsiella|cavit|man", "", x)))

abstract_corpus <- my_corpus

wordcloud(words = abstract_corpus,
          min.freq = 80, # Minimum word frequency to include
          scale = c(3, 0.5), # Word size scaling
          colors = brewer.pal(8, "Dark2")) # Color palette

```

```{r}
# transformació del Corpus en matriu de freqüències absolutes

m <- as.data.frame(t(as.matrix(TermDocumentMatrix(my_corpus))))
dim(m)
```

```{r, warning = FALSE}
set.seed(123)
hopkins(m)

# a value close to 1 tends to indicate the data is highly clustered
# so now we know that our data is highly clustered
```

```{r}
summary(clValid(m, 3:8, clMethods=c("hierarchical","kmeans","pam"), validation="internal"))
```

```{r}
# matriu de distàncies amb la distàncai de canberra

dist_matrix <- dist(x=m, method = "canberra")
```

Gradient TESS amb el mètode de kmeans:

```{r}
TESS <- fviz_nbclust(m, FUNcluster= kmeans, diss=NULL, k.max=15, method = "wss")

# gradient TESS:

gt<-data.frame()

gt[1,1]<-(TESS$data[1,2]-TESS$data[2,2])*100/TESS$data[1,2]
gt[2,1]<-(TESS$data[2,2]-TESS$data[3,2])*100/TESS$data[2,2]
gt[3,1]<-(TESS$data[3,2]-TESS$data[4,2])*100/TESS$data[3,2]
gt[4,1]<-(TESS$data[4,2]-TESS$data[5,2])*100/TESS$data[4,2]
gt[5,1]<-(TESS$data[5,2]-TESS$data[6,2])*100/TESS$data[5,2]
gt[6,1]<-(TESS$data[6,2]-TESS$data[7,2])*100/TESS$data[6,2]
gt[7,1]<-(TESS$data[7,2]-TESS$data[8,2])*100/TESS$data[7,2]
gt[8,1]<-(TESS$data[8,2]-TESS$data[9,2])*100/TESS$data[8,2]
gt[9,1]<-(TESS$data[9,2]-TESS$data[10,2])*100/TESS$data[9,2]

plot(2:10,gt[,1],type="b",xlab="clusters", ylab="GradTess",main="TESS")
```

Veiem que el nombre òptim de clusters seria 4.

Gradient TESS amb el mètode de PAM:

```{r}
TESS3 <- fviz_nbclust(m, FUNcluster= pam, diss=NULL, k.max=15, method = "wss")

# gradient TESS

gt<-data.frame()

gt[1,1]<-(TESS3$data[1,2]-TESS3$data[2,2])*100/TESS3$data[1,2]
gt[2,1]<-(TESS3$data[2,2]-TESS3$data[3,2])*100/TESS3$data[2,2]
gt[3,1]<-(TESS3$data[3,2]-TESS3$data[4,2])*100/TESS3$data[3,2]
gt[4,1]<-(TESS3$data[4,2]-TESS3$data[5,2])*100/TESS3$data[4,2]
gt[5,1]<-(TESS3$data[5,2]-TESS3$data[6,2])*100/TESS3$data[5,2]
gt[6,1]<-(TESS3$data[6,2]-TESS3$data[7,2])*100/TESS3$data[6,2]
gt[7,1]<-(TESS3$data[7,2]-TESS3$data[8,2])*100/TESS3$data[7,2]
gt[8,1]<-(TESS3$data[8,2]-TESS3$data[9,2])*100/TESS3$data[8,2]
gt[9,1]<-(TESS3$data[9,2]-TESS3$data[10,2])*100/TESS3$data[9,2]

plot(2:10,gt[,1],type="b",xlab="clusters", ylab="GradTess",main="TESS")
```

Veiem que el nombre òptim de clusters seria 4.

Seguidament aplicarem el mètode PAM:

```{r}
k <- 4
pam.k <- pam(dist_matrix,k,diss=T)

fviz_cluster(list(data = m, cluster = pam.k$clustering),ellipse.type = "convex", repel = TRUE, show.clust.cent = TRUE, ggtheme = theme_minimal())
```

Les primeres paraules de cada clusters:

```{r, warning=FALSE}
for (i in 1:k) {
 cat("20 more repeated words in cluster ",i," : ", names(sort(colSums(m[pam.k$clustering==i,]), decreasing = TRUE)[1:20]),"\n") 
}
```


```{r, warning=FALSE}
png("wordclouds_clusters.png", width = 500, height = 500)
par(mfrow=c(2,2))
for (i in 1:k) {
  word_frequencies <- colSums(m[pam.k$clustering==i,])
  word_freq_df <- data.frame(word = names(word_frequencies), freq = word_frequencies)

  wordcloud(words = word_freq_df$word,
            freq = word_freq_df$freq,
            min.freq = 50, # Minimum word frequency to include
            scale = c(3, 0.5), # Word size scaling
            colors = brewer.pal(8, "Dark2")) # Color palette
  title_wordclouds <- paste("WORDCLOUD OF CLUSTER", as.character(i))
  title(title_wordclouds)
}
dev.off()
```

Mostrem la infromació dels centroides de l'anàlisis dels àbstracts:

```{r}
df_abstracts[pam.k$medoids,]
```

To expose for the project

```{r}
png("wordclouds_titles_abstracts.png", width = 700, height = 350)
par(mfrow = c(1, 2))
wordcloud(words = title_corpus,
          min.freq = 10, # Minimum word frequency to include
          scale = c(3, 0.5), # Word size scaling
          colors = brewer.pal(8, "Dark2")) # Color palette
title("TITLES WORDCLOUD")

wordcloud(words = abstract_corpus,
          min.freq = 80, # Minimum word frequency to include
          scale = c(3, 0.5), # Word size scaling
          colors = brewer.pal(8, "Dark2")) # Color palette
title("ABSTRACTS WORDCLOUD")
dev.off()
```






